# -*- coding: utf-8 -*-
"""scrapping_coursera.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1VUrzrnvtBcqlQ6OtaShGTbAuzlsttH9Z
"""

from bs4 import BeautifulSoup
import requests
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.common.by import By
from selenium.webdriver.support import expected_conditions as EC
import time

def get_page_source(url):
  options = Options()
  options.add_argument("--window-size=4000x4000")
  options.add_argument("--headless")
  options.add_experimental_option("detach", True)
  driver = webdriver.Chrome(options=options)
  driver.get(url)
  time.sleep(3)
  source = driver.page_source
  return source, driver

def scrape(columns):
  url = "https://www.personality-database.com/top-story"
  source, _ = get_page_source(url)
  _.quit()
  soup = BeautifulSoup(source, 'html.parser')
  story_area = soup.find_all("div", class_ = "story-area")[5]
  anime_list = story_area.find_all("a")
  for anime in anime_list:
    href = anime["href"]
    name = anime.find("div", class_ = "name").get_text()
    group_name = anime.find("div", class_ = "group-name").get_text()
    scrape_anime_data(href, name, group_name, columns)

def download_and_save_image(url, folder, file_name):
  img_data = requests.get(url).content
  with open(f'./data/{folder}/{file_name}.jpg', 'wb') as handler:
      handler.write(img_data)

def scrape_page_data(soup, name, group_name, columns):
  profile_cards = soup.find_all("div", class_ = "profile-card")
  for profile in profile_cards:
    character_name = profile.find("h2", class_ = "info-name").get_text()
    try:
      avatar_url = profile.find("div", class_ = "avatar").find("img")["src"]
      download_and_save_image(avatar_url, name, character_name)
    except:
      pass
    character_mbti_type = profile.find("div", class_ = "personality").get_text()
    character_enneagram_type = profile.find("div", class_ = "subtype").get_text()
    columns[0].append(name)
    columns[1].append(group_name)
    columns[2].append(character_name)
    columns[3].append(character_mbti_type)
    columns[4].append(character_enneagram_type)

import os

def scrape_anime_data(href, name, group_name, columns):
  url = "https://www.personality-database.com" + href
  source, driver = get_page_source(url)
  soup = BeautifulSoup(source, 'html.parser')
  clean_name = name
  if name.find(":") != -1:
    clean_name = name.replace(":", " ")
  os.makedirs(f"./data/{clean_name}",exist_ok = True)
  scrape_page_data(soup, name, group_name, columns)
  for i in range(1, 6):
    print(f"Scraping {name}: Page {i}")
    element = driver.find_element(By.CLASS_NAME, "rc-pagination").find_elements(By.TAG_NAME, "li")
    element[-2].click()
    time.sleep(3)
    soup = BeautifulSoup(driver.page_source, 'html.parser')
    scrape_page_data(soup, name, group_name, columns)
  driver.quit()
       
# the function scrape is used to get the data

anime_name = []
anime_genre = []
character_name = []
character_mbti_type = []
character_enneagram_type = []

scrape([anime_name, anime_genre, character_name, character_mbti_type, character_enneagram_type])

# making an empty list so that we can append each of them at the end into a list for making dataframe.

import pandas as pd

data_table = {
             "anime_name": anime_name,
             "anime_genre": anime_genre,
             "character_name": character_name,
             "character_mbti_type": character_mbti_type,
             "character_enneagram_type": character_enneagram_type,
              }

df = pd.DataFrame(data_table)
df.to_csv(f'anime_character_personality.csv')

# here we take each lists we generated by scrapping and made a dataframe out of it isung pandas library.

# At end we convert it into a csv file so we can use it for our data analysis part.
